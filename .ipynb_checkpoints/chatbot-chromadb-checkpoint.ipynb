{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing the libraries:\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading the pdf from the folder:\n",
    "loader = PyPDFLoader(\"HR-DigivateLabs-Leave-Policy.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading the huggingface api key:\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(chunk_size,chunk_overlap,temperature,query):\n",
    "    #splitting into chunks:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                   chunk_overlap=chunk_overlap)\n",
    "    final_document = text_splitter.split_documents(documents)\n",
    "    \n",
    "    #initializing embedding technique:\n",
    "    hugging_face_embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "        model_kwargs={'device':'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings':True}\n",
    "    )\n",
    "    \n",
    "    #creating the vector store:\n",
    "    vector_store = LanceDB.from_documents(final_document[:],hugging_face_embeddings)\n",
    "    \n",
    "    #creating a retriever object:\n",
    "    retriever = vector_store.as_retriever(search_type='similarity',\n",
    "                                      search_kwargs={\"k\":3})\n",
    "    \n",
    "    #creating a prompt template:\n",
    "    template = \"\"\"You are a knowledgeable assistant trained to provide accurate answers based on the information \n",
    "    contained in the context. When a user asks you a question, your task is to:\n",
    "\n",
    "    1. Carefully analyze the question.\n",
    "    2. Search for the relevant information and summarize it.\n",
    "    3. If the answer is found, respond with the complete and concised information.\n",
    "    4. If the answer is not found in the document, respond with \"I don't know.\"\n",
    "\n",
    "    Question: {question} ,\n",
    "    Context: {context},\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template,\n",
    "                            input_variables=[\"context\",\"question\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #loading a hugging face model:\n",
    "    llm = HuggingFaceHub(\n",
    "            repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "            model_kwargs={\"temperature\":temperature,\n",
    "                          \"max_length\":500}\n",
    "    )\n",
    "    \n",
    "    #creating a retireval QA:\n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\":prompt}\n",
    "    )\n",
    "    \n",
    "    #testing the model with a query:\n",
    "    response = retrievalQA.invoke({\"query\":query})\n",
    "\n",
    "    # Fetching only the context from the response:\n",
    "    context = response['source_documents'][0].page_content\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process  \n",
      " \n",
      "o To avail maternity, the employee must notify the respective line manager of her intent to take \n",
      "maternity leave preferably no later than 15 weeks prior to the date of delivery and apply the leaves in \n",
      "Payasia.A scanned copy of the maternity certificate confi rming the date of delivery and relevant \n",
      "medical documents must be submitted to HR Deptt.  \n",
      "o HR Deptt will respond to an employeeâ€Ÿs notification of leave plans within 28 days of being notified \n",
      "with the details of the expected return to work if the employee tak es her full entitlement.  \n",
      "o In the event that childbirth occurs before the employee is due to commence maternity leave, such \n",
      "maternity leave will automatically start from the date of childbirth.  \n",
      "o The employee is expected to provide a declaration in writing sta ting that the employee will not work in \n",
      "any other establishment during the period for which maternity benefits are being received. Violation of \n",
      "this policy/ clause will be viewed as serious breach of company policy which may result in severe \n",
      "disciplinary a ction including termination of employment.\n"
     ]
    }
   ],
   "source": [
    "#testing the function:\n",
    "chunk_size=1200\n",
    "chunk_overlap=480\n",
    "temperature=1\n",
    "\n",
    "query=\"What is the process to apply for maternity leave?\"\n",
    "\n",
    "response = get_response(chunk_size,chunk_overlap,temperature,query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR-Digivate Labs -Leave-Policy \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "All eligible women employees are entitled to maternity leave, as shown in the table below. The maternity \n",
      "leave is inclusive of weekly offs and public & national holidays.  \n",
      " \n",
      "Types of Maternity \n",
      "Leaves  Leave Entitlement \n",
      "(In Weeks)  Documents required to be \n",
      "submitted to HR Deptt to \n",
      "avail the leave  Leave \n",
      "Commencement  \n",
      "Maternity leave in case \n",
      "of women employee up \n",
      "to two surviving children  26 1. Confirmation of pregnancy \n",
      "along with t he date of delivery.  \n",
      "2. Medical certificate from \n",
      "certified medical practitioner.  Not earlier than 8 \n",
      "weeks prior to the \n",
      "date of delivery.  \n",
      "Maternity leave in case \n",
      "of women employee with \n",
      "two or more children  12 1. Confirmation of pregnancy \n",
      "along with the date of delivery.  \n",
      "2. Medical certificate from \n",
      "certified medical practitioner.  Not earlier than 6 \n",
      "weeks prior to the \n",
      "date of delivery.  \n",
      "Commissioning Mother  12 1. Medical Documents  \n",
      "2. Birth certificate of the ch ild From the date the \n",
      "child is handed over \n",
      "to the commissioning \n",
      "mother after birth.  \n",
      "Leave for \n",
      "miscarriage/medical \n",
      "termination  4 Medical certificate from certified \n",
      "medical practitioner.  Day of the \n",
      "miscarriage/ medical\n"
     ]
    }
   ],
   "source": [
    "#testing the function:\n",
    "chunk_size=1200\n",
    "chunk_overlap=480\n",
    "temperature=1\n",
    "\n",
    "query=\"What are the Types of Maternity Leaves?\"\n",
    "\n",
    "response = get_response(chunk_size,chunk_overlap,temperature,query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave of absence can be availed only after completing 3 months of service from date of joining. Employees \n",
      "are encouraged to avail their leave within a cal endar year. Leave taken will be calculated based on Working \n",
      "Days only. Leave of absence can be availed in multiple of 1 day only.  Short Duration Leave (up to 2 days at a \n",
      "stretch, minimum  1 day ) can be availed of with prior intimation to Reporting Manager, followed by formal \n",
      "regularization  as mentioned  earlier.\n"
     ]
    }
   ],
   "source": [
    "#testing the function:\n",
    "chunk_size = 600\n",
    "chunk_overlap = 150\n",
    "temperature = 1\n",
    "\n",
    "query=\"When can I avail my leave of absence?\"\n",
    "\n",
    "response = get_response(chunk_size,chunk_overlap,temperature,query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carry Forward  \n",
      " \n",
      "You can carry forward a maximum of 7 leaves to a new calendar year. Thus, your leave balance cannot \n",
      "exceed 22 days at any given time. For instance, if you have accumula ted 7 days of leave by the end of a year \n",
      "and have added 13 days by 1st December, your leave balance will be 20. However, if you utilize 10 days \n",
      "during December, your leave balance as on 1st January will still be 7 only.\n"
     ]
    }
   ],
   "source": [
    "#testing the function:\n",
    "chunk_size=420\n",
    "chunk_overlap=120\n",
    "temperature=1\n",
    "\n",
    "query=\"How many leaves can we carry forward?\"\n",
    "\n",
    "response = get_response(chunk_size,chunk_overlap,temperature,query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Test no 2\n",
       "### Changing the templates, adding functions\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## Test no 2\n",
    "### Changing the templates, adding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_test2(chunk_size,chunk_overlap,temperature,query,template):\n",
    "    #splitting into chunks:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                   chunk_overlap=chunk_overlap)\n",
    "    final_document = text_splitter.split_documents(documents)\n",
    "    \n",
    "    #initializing embedding technique:\n",
    "    hugging_face_embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "        model_kwargs={'device':'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings':True}\n",
    "    )\n",
    "    \n",
    "    #creating the vector store:\n",
    "    vector_store = LanceDB.from_documents(final_document[:],hugging_face_embeddings)\n",
    "    \n",
    "    #creating a retriever object:\n",
    "    retriever = vector_store.as_retriever(search_type='similarity',\n",
    "                                      search_kwargs={\"k\":3})\n",
    "\n",
    "    prompt = PromptTemplate(template=template,\n",
    "                            input_variables=[\"context\",\"question\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #loading a hugging face model:\n",
    "    llm = HuggingFaceHub(\n",
    "            repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "            model_kwargs={\"temperature\":temperature,\n",
    "                          \"max_length\":500}\n",
    "    )\n",
    "    \n",
    "    #creating a retireval QA:\n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\":prompt}\n",
    "    )\n",
    "    \n",
    "    #testing the model with a query:\n",
    "    response = retrievalQA.invoke({\"query\":query})\n",
    "\n",
    "    # Fetching only the context from the response:\n",
    "    context = response['source_documents'][0].page_content\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_relevant_text(context, query):\n",
    "    # Convert query and context to lower case for case-insensitive matching\n",
    "    query = query.lower()\n",
    "    sentences = context.split('. ')\n",
    "    \n",
    "    # Use regular expressions to match the most relevant sentences\n",
    "    relevant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if similarity(query, sentence.lower()) > 0.2:  # You can adjust the threshold\n",
    "            relevant_sentences.append(sentence)\n",
    "\n",
    "    # Rank the sentences based on similarity to the query\n",
    "    relevant_sentences = sorted(relevant_sentences, key=lambda x: similarity(query, x.lower()), reverse=True)\n",
    "    \n",
    "    # Limit to the top 2-3 sentences for a more precise response\n",
    "    return '. '.join(relevant_sentences) + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Carry Forward  \n",
      " \n",
      "You can carry forward a maximum of 7 leaves to a new calendar year.  \n",
      " \n",
      "Maternity Leave  \n",
      " \n",
      "Definitions. Thus, your leave balance cannot \n",
      "exceed 22 days at any given time. However, if you utilize 10 days \n",
      "during December, your leave balance as on 1st January will still be 7 only. HR-Digivate Labs -Leave-Policy \n",
      "  \n",
      " \n",
      "The above policy shall be applicable to all full time/contract Employees.  \n",
      " \n",
      "Encashment  \n",
      " \n",
      "NO ENCASHMENT OF LEAVE.  \n",
      " \n",
      "Short duration leaves should only be approved by HR after consideration or BU Head and can be done only \n",
      "once or twice in a month.  \n",
      " \n",
      "Casual  & Sick Leave  \n",
      " \n",
      "NO SEPARATE CASUAL or SICK LEAVE, ALL LEAVES COMBINED INTO ONE COMMON POOL AS \n",
      "ACCRUED LEAVE . For instance, if you have accumula ted 7 days of leave by the end of a year \n",
      "and have added 13 days by 1st December, your leave balance will be 20. Un -availed leave may be adjusted at the time of separation, at the sole \n",
      "discretion  of the management.\n"
     ]
    }
   ],
   "source": [
    "#testing the function:\n",
    "chunk_size=1200\n",
    "chunk_overlap=480\n",
    "temperature=1\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "  Use the following retrieved context to answer the question as briefly and clearly as possible:\n",
    "  {context}\n",
    " \n",
    "  If you don't know the answer, just say that you don't know.\n",
    "  Please provide a short and direct answer.\n",
    "  Question: {question}\n",
    "  Answer: \"\"\"\n",
    "\n",
    "\n",
    "query=\"How many leaves can I carry forward?\"\n",
    "\n",
    "\n",
    "context = get_response_test2(chunk_size,chunk_overlap,temperature,query,template)\n",
    "response = extract_relevant_text(context,query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Saving the model\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('PDF_ChatBot_RatHat/models',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /opt/app-root/src/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_hugging_face_token' with your actual token\n",
    "login(token=os.environ['HUGGINGFACEHUB_API_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install optimum[exporters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --model mistralai/Mistral-7B-v0.1 PDF_ChatBot_RatHat/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORTModelForSequenceClassification\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimum'"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "save_directory = \"PDF_ChatBot_RatHat/models/\"\n",
    "\n",
    "# Load a model from transformers and export it to ONNX\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Save the onnx model and tokenizer\n",
    "ort_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the FAISS index\n",
    "import faiss\n",
    "faiss.write_index(vector_store.index, \"faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7f9a30b53480> >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
